{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:50:12.330314Z",
     "start_time": "2019-07-18T16:50:12.321323Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Chain:\n",
    "    def __init__(self, chain_words):\n",
    "        self.chain_words = chain_words\n",
    "        self.predicted = {}\n",
    "        self.total_count = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word in self.predicted:\n",
    "            self.predicted[word]['count'] += 1\n",
    "        else:\n",
    "            self.predicted[word] = {'count': 1, 'proba': 0}\n",
    "        self.total_count += 1\n",
    "        self.compute_proba()\n",
    "        self.predicted = OrderedDict(sorted(self.predicted.items(), key=lambda i: i[1]['proba'], reverse=True))\n",
    "    \n",
    "    def compute_proba(self):\n",
    "        for key, word in self.predicted.items():\n",
    "            proba = word['count'] / self.total_count\n",
    "            self.predicted[key]['proba'] = proba\n",
    "    \n",
    "    def generate_word(self):\n",
    "        treshold = random.random()\n",
    "        for key, word in self.predicted.items():\n",
    "            if treshold <= word['proba']:\n",
    "                return key\n",
    "            else:\n",
    "                # egg: if th = 0.8, 1st proba = 0.5 and 2nd = 0.3, we want th to be = to 0.3 when we compare to 2nd prob\n",
    "                treshold -= word['proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:50:14.973074Z",
     "start_time": "2019-07-18T16:50:14.969065Z"
    }
   },
   "outputs": [],
   "source": [
    "c = Chain(['Je', 'suis', 'très'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:50:16.964516Z",
     "start_time": "2019-07-18T16:50:16.960319Z"
    }
   },
   "outputs": [],
   "source": [
    "c.add_word('test')\n",
    "c.add_word('heureux')\n",
    "c.add_word('triste')\n",
    "c.add_word('heureux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:06:21.949701Z",
     "start_time": "2019-07-18T16:06:21.942694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('heureux', {'count': 2, 'proba': 0.5}),\n",
       "             ('test', {'count': 1, 'proba': 0.25}),\n",
       "             ('triste', {'count': 1, 'proba': 0.25})])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:50:22.890006Z",
     "start_time": "2019-07-18T16:50:22.884021Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_chain(chain, samples=1000):\n",
    "    from collections import Counter\n",
    "    res = {}\n",
    "    for i in range(samples):\n",
    "        w = chain.generate_word()\n",
    "        res[w] = res[w] + 1 if w in res else 1\n",
    "    # lets compute the error\n",
    "    print({k:x/samples - chain.predicted[k]['proba'] for (k,x) in Counter(res).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T16:10:18.664506Z",
     "start_time": "2019-07-18T16:10:13.240984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heureux': -4.300000000001525e-05, 'triste': -0.0003120000000000067, 'test': 0.0003549999999999942}\n"
     ]
    }
   ],
   "source": [
    "test_chain(c, samples=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:43:35.480022Z",
     "start_time": "2019-08-02T12:43:35.457708Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Markov:\n",
    "    def __init__(self, chain_length, corpus=None, tokenizer=None, sentence_delimiter=\".\"):\n",
    "        self.chain_length = chain_length\n",
    "        self.chains = {}\n",
    "        self.default_punkt = sentence_delimiter\n",
    "        if corpus:\n",
    "            self.parse_corpus(corpus, tokenizer)\n",
    "            \n",
    "    def parse_corpus(self, corpus, tokenizer=None):\n",
    "        import re\n",
    "        # tokenize into sentences\n",
    "        # if no tokenizer we assume corpus should be an array of sentences (array of words)\n",
    "        if tokenizer is None:\n",
    "            sentences = corpus\n",
    "        elif tokenizer == \"punkt\":\n",
    "            import nltk\n",
    "            sentences = nltk.sent_tokenize(corpus)\n",
    "            # now array of sentences\n",
    "            # lets transform sentences into words\n",
    "            sentences = [re.sub(r\"[^\\w\\d'\\s]+\",'', s.lower()).split() for s in sentences]\n",
    "            \n",
    "        # custom method passed? \n",
    "        else:\n",
    "            sentences = tokenizer(corpus)\n",
    "            \n",
    "        # add start and stop words\n",
    "        sentences = [[\"<s>\"] + s + [\"</s>\"] for s in sentences]\n",
    "                \n",
    "        # train for every sentence\n",
    "        for s in sentences:\n",
    "            for i in range(self.chain_length, len(s)):\n",
    "                predicted = s[i]\n",
    "                chain_words = s[i - self.chain_length:i]\n",
    "                chain_id = \"@ç@\".join(chain_words)\n",
    "                if chain_id not in self.chains:\n",
    "                    self.chains[chain_id] = Chain(chain_words)\n",
    "                self.chains[chain_id].add_word(predicted)\n",
    "    \n",
    "    def generate_sentence(self, start=None):\n",
    "        if start:\n",
    "            # tokenize start\n",
    "            start = [ w.lower() for w in start.split()]\n",
    "        # if not start generate a random start\n",
    "        if not start or \"@ç@\".join(start) not in self.chains:\n",
    "            start = random.choice(self.find_chain(\"<s>\")).split('@ç@')\n",
    "        res = deque(start)\n",
    "        chain = deque(start)\n",
    "        word = \"\"\n",
    "        # generate words until reaching a stop word\n",
    "        while word != \"</s>\":\n",
    "            cid = '@ç@'.join(chain)\n",
    "            if cid not in self.chains:\n",
    "                return (\" \".join(res)).strip()\n",
    "            word = self.chains[cid].generate_word()\n",
    "            res.append(word)\n",
    "            chain.append(word)\n",
    "            chain.popleft()\n",
    "        \n",
    "        punctuation = self.default_punkt\n",
    "        # remove start and stop words\n",
    "        res.pop()\n",
    "        if res[0] == \"<s>\":\n",
    "            res.popleft()\n",
    "        if res[0] in [\"quoi\", 'comment', 'quel', 'lequel', 'pourquoi', 'quand', 'est', 'qui', 'où']:\n",
    "            punctuation = \"?\"\n",
    "        sentence = (\" \".join(res)).strip()\n",
    "        sentence = sentence[0].upper() + sentence[1:]\n",
    "        return sentence + punctuation + \" \"\n",
    "        \n",
    "    def generate(self, nb_sentences=10, start=None, link_chains=False):\n",
    "        res = \"\"\n",
    "        for n in range(nb_sentences):\n",
    "            sentence = self.generate_sentence(start)\n",
    "            res += sentence\n",
    "            if link_chains:\n",
    "                start = \" \".join(sentence[len(sentence) - self.chain_length:])\n",
    "            else:\n",
    "                start = None\n",
    "        return res.strip()\n",
    "    \n",
    "    def find_chain(self, searched):\n",
    "        res = []\n",
    "        for key in self.chains.keys():\n",
    "            if searched in key:\n",
    "                res.append(key)\n",
    "        return res\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:54:39.014232Z",
     "start_time": "2019-08-02T12:54:38.980085Z"
    }
   },
   "outputs": [],
   "source": [
    "mousquetaires = open('./data/3mousq.txt', 'r').read().strip().replace('\\n', ' ').replace('#', '').replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233558"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mousquetaires.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mousquetaires_2 = Markov(2, mousquetaires, 'punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Donnonsnous ce plaisir d'ailleurs ce retour lui offrait un avantage c'était de surveiller lui même le départ de mon évanouissement felton écoutait ce dialogue sans dire gare. Montezen six.\""
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_mousquetaires_2.generate(2, link_chains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:54:43.181184Z",
     "start_time": "2019-08-02T12:54:41.706124Z"
    }
   },
   "outputs": [],
   "source": [
    "m_mousquetaires_3 = Markov(3, mousquetaires, \"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-02T12:57:11.741246Z",
     "start_time": "2019-08-02T12:57:11.583671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Athos porthos et aramis se placèrent à une table et se mit à table mangea peu et ne but que de l'eau. À mon tour.\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_mousquetaires_3.generate(2, \"athos porthos et\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mousquetaires_4 = Markov(4, mousquetaires, \"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"L'un des deux gardes était invité pour le soir même mais nous devons dire à la louange de m d'artagnan fils quelques efforts qu'il tentât pour rester ferme comme le devait être un futur mousquetaire la nature l'emporta et il versa force larmes dont il parvint à grandpeine à cacher la moitié. Mais en venant par trop matin je crains de réveiller votre majesté.\""
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_mousquetaires_4.generate(2, link_chains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr = open('./data/lotr.txt', 'r').read().strip().replace('\\n', ' ').replace('#', '').replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lotr_2 = Markov(2, lotr, 'punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skulls and bones black in cinders lie beneath the roots of the food i send with you. Drea dful as the valley.'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lotr_2.generate(2, link_chains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lotr_3 = Markov(3, lotr, 'punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The rain's nearly given over already' said sam 'but i wouldn't be one to say that the journey went well and they met no danger and heard nothing and seen nothing for two nights now'. 'the red arrow'.\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lotr_3.generate(2, link_chains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lotr_4 = Markov(4, lotr, 'punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As dusk drew down on the fourth day that they rode still forward after dusk and into the night beneath the moon. An author cannot of course remain wholly unaffected by his experience but the ways in which a storygerm uses the soil of experience are extremely complex and attempts to define the process are at best guesses from evidence that is inadequate and ambiguous.'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_lotr_4.generate(2, link_chains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "m = Markov(3, sentence_delimiter=\"\")\n",
    "corpus = []\n",
    "for tab in os.listdir('./scripts/parsed_tabs_folk'):\n",
    "    with open(os.path.join('./scripts/parsed_tabs_folk', tab)) as f:\n",
    "        t = json.load(f)\n",
    "        t = [x for x in t if x != '------']\n",
    "        corpus.append(t)\n",
    "\n",
    "print(len(corpus))\n",
    "m.parse_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--0-------------------------------0---------------------------------------------2--------------------\n",
      "|--1---0-1-4-------0---------2h3-------3p1-0-0---0-1p0-------3---3-------3---3------------------\n",
      "|--2-2---------0-2---------------------0-----------3-----------2---------2---2-------2---2------------\n",
      "|--2---------4-----2p1h2-4-----------2---------0---------0-------0-----0-----------0-----0--------\n",
      "|0-0-------2-------------------------3-----------------2---------0-----0-------------------4---2-0----\n",
      "|----------------------------------------------3---------------------------------------------------3-3\n"
     ]
    }
   ],
   "source": [
    "# res = m.generate(1, start=\"---3-- ---2-- ---0--\")\n",
    "res = m.generate(1)\n",
    "# [{'id': c.chain_words, 'pred': c.predicted} for c in m.chains.values()]\n",
    "tab = \"\"\n",
    "strings = {0:\"|\", 1:\"|\", 2:\"|\", 3:\"|\", 4:\"|\", 5:\"|\"}\n",
    "words = res.split(' ')\n",
    "for index, w in enumerate(words):\n",
    "    if len(w) == 6:\n",
    "        for i in range(6):\n",
    "            try:\n",
    "                strings[i] += w[i] + \"-\" if index < len(words) - 1 and words[index + 1][i] not in [\"p\", \"h\", \"b\"] and w[i] not in ['p', 'h', 'b'] else w[i]\n",
    "            except IndexError:\n",
    "                print(w, i, index)\n",
    "    else:\n",
    "        for i in range(6):\n",
    "            try:\n",
    "                strings[i] += w[i * 2: i * 2 + 2] + \"-\" if index < len(words) - 1 and words[index + 1][i] not in [\"p\", \"h\", \"b\"] and w[i] not in ['p', 'h', 'b'] else w[i * 2: i * 2 + 2]\n",
    "            except IndexError:\n",
    "                print(w, i, index)\n",
    "print(\"\\n\".join(strings.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
